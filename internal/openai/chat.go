package openai

import "strings"

const CHAT_COMPLETION_URL = "https://api.openai.com/v1/chat/completions"

type ChatCompletionRequestBody struct {
	// A list of messages comprising the conversation so far.
	Messages []ChatCompletionRequestMessage `json:"messages"`
	// ID of the model to use. You can use the List models API
	// to see all of your available models, or see our Model overview
	// for descriptions of them.
	Model string `json:"model"`
	// What sampling temperature to use, between 0 and 2.
	// Higher values like 0.8 will make the output more random,
	// while lower values like 0.2 will make it more focused and deterministic.
	// We generally recommend altering this or top_p but not both.
	Temperature float64 `json:"temperature"`
	// An alternative to sampling with temperature.
	// An alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	// Generally recommend altering this or temperature but not both.
	TopP float64 `json:"top_p"`
	// How many completions to generate with each prompt
	N int64 `json:"n"`
	// Whether or not to stream back partial progress. If set, tokens will be sent
	// ass data-only server-sent events as they become available. with the Stream
	// terminated by a data: [Done] message.
	Stream bool `json:"stream"`
	// The maximum number of tokens to generate in the completion.
	// The token count of your prompt plus max_tokens cannot exceed
	// the model's context length. Most models have a context length
	// of 2048 tokens (except for the newest models, which support 4096).
	MaxTokens int64 `json:"max_tokens"`
	// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
	// in the text so far, increasing the model's likelihood to talk about new topics.
	PresencePenalty float64 `json:"presence_penalty"`
	// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing Frequency
	// in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty float64 `json:"frequency_penalty"`
	// Modify the likelihood of specified tokens appearing in the completion.
	// Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer)
	// to an associated bias value from -100 to 100. You can use this tokenizer tool
	// (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically,
	// the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase
	// likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection
	// of the relevant token.
	// As an example, you can pass {"50256": -100} to prevent the <|endoftext|> token from being generated.
	LogitBias map[string]int64 `json:"logit_bias"`
	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User string `json:"user"`
	/* An object specifying the format that the model must output.Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.

	Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if finish_reason="length", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.
	*/
	ResponseFormat ChatCompletionRequestResponseFormat `json:"response_format"`
	Tools          []ChatCompletionTool                `json:"tools"`
	// This will either be a string: `auto` or `none`. OR
	// This will be an object {"type": "function", "function": {name: "my_function"}}
	// Also similar to ChatCompletionTool
	ToolChoices any `json:"tool_choice"`
}

type ChatCompletionRequestBaseMessage struct {
	// contents of the message
	Content string `json:"content"`
	// The role of the messages author.
	Role string `json:"role"`
	// Optional. Name for the participant. Provides the model
	// information to differentiate between participants of
	// the same role.
	Name string `json:"name"`
}

type ChatCompletionRequestSystemMessage struct {
	ChatCompletionRequestBaseMessage
}

type ChatCompletionRequestUserMessage struct {
	ChatCompletionRequestBaseMessage
}

type ChatCompletionRequestAssistantMessage struct {
	ChatCompletionRequestBaseMessage
	ToolCalls []ChatCompletionToolCall `json:"tool_calls"`
}

type ChatCompletionRequestToolMessage struct {
	ChatCompletionRequestBaseMessage
	ToolCallId string `json:"tool_call_id"`
}

type ChatCompletionRequestMessage struct {

	// The role of the message.
	// Examples: system, user, assistant
	Role string `json:"role,omitempty"`
	// The actual prompt. This is equivalent of the prompt would be for the
	// regular chat completion.
	Content string `json:"content,omitempty"`
	// ToolCalls generated by the model, such as function calls.
	// This replaces the old function_call field.
	ToolCalls []ChatCompletionToolCall `json:"tool_calls,omitempty"`
}

type ChatCompletionToolCall struct {
	// The ID of the tool call.
	Id string `json:"id,omitempty"`
	// The type of the tool. Currently only `function` is supported.
	Type string `json:"type,omitempty"`
	// The Function the Model Called.
	Function ChatCompletionFunctionCall `json:"function,omitempty"`
}

type ChatCompletionFunctionCall struct {
	// The name of the function to call
	Name string `json:"name,omitempty"`
	// The arguments to call the function with, as generated by the model in JSON format.
	// Note that the model does not always generate valid JSON, and may hallucinate
	// parameters not defined by your function schema. Validate the args in
	// your code efore calling your function.
	Arguments string `json:"arguments,omitempty"`
}

type ChatCompletionRequestFunction struct {
	Description string         `json:"description"`
	Name        string         `json:"name"`
	Parameters  map[string]any `json:"parameters"`
}

type ChatCompletionTool struct {
	Type     string                        `json:"type"`
	Function ChatCompletionRequestFunction `json:"function"`
}

const (
	ResponseFormatTypeText       = "text"
	ResponseFormatTypeJsonObject = "json_object"
)

type ChatCompletionRequestResponseFormat struct {
	// Optional. Must be one of:
	// text or json_object.
	Type string `json:"type"`
}

// The Response Returned by OpenAI
type ChatCompletionResponse struct {
	// Id of the completion
	Id string `json:"id"`
	// Type of object
	Object string `json:"object"`
	// Unix timestamp
	Created int64 `json:"created"`
	// Model used
	Model string `json:"model"`
	// A list of chat completion choices. Can be more than 1 if
	// n > 1.
	Choices []ChatCompletionChoice `json:"choices"`
	// This fingerprint represents the backend configuration that
	// the model runs with. Can be used in conjunction with the
	// seed request parameter to understand when the backend changes
	// have been made that might impact determinism.
	SystemFingerprint string `json:"system_fingerprint"`
	// Usage report
	Usage ChatCompletionUsage `json:"usage"`
}

type ChatCompletionChunkedResponse struct {
	Id string `json:"id"`
	// A list of chat completion choices. Can be more than 1 if
	// n > 1.
	Choices []ChatCompletionChunkedChoiceDelta `json:"choices"`
	// The unix timestamp (in seconds) of when the chat completion was created.
	// Each chunk has the same timestamp.
	Created int64 `json:"created"`
	// The model that generated the completion.
	Model string `json:"model"`
	// This fingerprint represents the backend configuration that the model runs with.
	// Can be used in conjunction with the seed request parameter to understand when backend
	// changes have been made that might impact determinism.
	SystemFingerPrint string `json:"system_fingerprint"`
	// The object type, which is always:
	// chat.completion.chunk
	Object string `json:"object"`
}

type ChatCompletionUsage struct {
	// Number of tokens in the prompt.
	PromptTokens int64 `json:"prompt_tokens"`
	// Number of tokens in the generated completion.
	CompletionTokens int64 `json:"completion_tokens"`
	// Total number of tokens in the prompt and completion.
	TotalTokens int64 `json:"total_tokens"`
}

type ChatCompletionChunkedChoiceDelta struct {
	Delta        ChatCompletionRequestMessage
	FinishReason string `json:"finish_reason"`
	Index        int64  `json:"index"`
}

type ChatCompletionChoice struct {
	// The text of the response
	Role string `json:"role"`
	// Index of the choice in the list.
	Index int64 `json:"index"`
	// The message response in the choice
	Message ChatCompletionRequestMessage
	// The reason the model stopped generating tokens. This will be stop if the model hit a natural
	// stop point or a provided stop sequence, length if the maximum number of tokens specified in
	// in the request was reached, content_filter if content was omitted due to a flag from our content
	// filters, tool_alls if the model called a tool, or function_call (deprecated) if the model
	// called a function.
	FinishReason string `json:"finish_reason"`
}

// Trim the newline character and question mark from the response
// for a clean presentation.
func (r *ChatCompletionResponse) CleanText() string {
	text := r.Choices[0].Message.Content
	text = strings.TrimLeft(text, "?\n")
	return text
}
